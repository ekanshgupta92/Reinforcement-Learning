{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f29a1f02",
   "metadata": {},
   "source": [
    "## REINFORCE Algorithm\n",
    "\n",
    "- Type of <font color='#00ba47'>policy gradient based method</font> (ie directly computes the policy without calculating the value function)\n",
    "- It learns a <font color='#00ba47'>stochastic policy</font> and our neural network’s output is an action vector that represents a probability distribution (rather than returning a single deterministic action).\n",
    "- Therefore REINFORCE selects an action from this probability distribution, ie if our Agent ends up in the same state twice, we may not end up taking the same action every time\n",
    "- The method REINFORCE is built upon trajectories instead of episodes because maximizing expected return over trajectories (instead of episodes) lets the method search for optimal policies for both episodic and continuing tasks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1eaa7ca3",
   "metadata": {},
   "source": [
    "Expected Return:\n",
    "$$U(\\theta) = \\sum_{\\tau} P(\\tau; \\theta)R(\\tau)$$ \n",
    "or\n",
    "$$\\nabla_{\\theta}U(\\theta) = \\sum_{t=0}^{H}\\nabla_{\\theta} \\log \\pi_{\\theta}(a_t | s_t) G_{t}$$ \n",
    "$$\\theta = \\theta + \\alpha \\nabla_{\\theta}U(\\theta)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7697c2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV = \"CartPole-v1\"\n",
    "REWARD_MAX = 195"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec567ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "torch.manual_seed(0) # set random seed\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8721b87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make(ENV)\n",
    "device = torch.device(\"mps\" if torch.has_mps else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21e2673f",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_size = env.observation_space.shape[0] \n",
    "n_actions = env.action_space.n  \n",
    "\n",
    "class Policy(nn.Module):\n",
    "    def __init__(self, s_size=obs_size, h_size=16, a_size=n_actions):\n",
    "        super(Policy, self).__init__()\n",
    "        self.fc1 = nn.Linear(s_size, h_size)\n",
    "        self.fc2 = nn.Linear(h_size, a_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.softmax(x, dim=1)\n",
    "    \n",
    "    def act(self, state):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        probs = self.forward(state).cpu()\n",
    "        m = Categorical(probs)\n",
    "        action = m.sample()\n",
    "        return action.item(), m.log_prob(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4aa0dc28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/pytorch/lib/python3.9/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: 20.25\n",
      "Episode 200\tAverage Score: 46.92\n",
      "Episode 300\tAverage Score: 76.51\n",
      "Episode 400\tAverage Score: 85.48\n",
      "Episode 500\tAverage Score: 106.72\n",
      "Episode 600\tAverage Score: 119.34\n",
      "Episode 700\tAverage Score: 71.195\n",
      "Episode 800\tAverage Score: 87.87\n",
      "Episode 900\tAverage Score: 68.45\n",
      "Episode 1000\tAverage Score: 69.38\n"
     ]
    }
   ],
   "source": [
    "policy = Policy().to(device)\n",
    "optimizer = optim.Adam(policy.parameters(), lr=1e-2)\n",
    "\n",
    "def reinforce(n_episodes=1000, max_t=1000, gamma=1.0, print_every=100):\n",
    "    scores_deque = deque(maxlen=100)\n",
    "    scores = []\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        saved_log_probs = []\n",
    "        rewards = []\n",
    "        state, _ = env.reset()\n",
    "        for t in range(max_t):\n",
    "            action, log_prob = policy.act(state)\n",
    "            saved_log_probs.append(log_prob)\n",
    "            state, reward, done, _, info = env.step(action)\n",
    "            rewards.append(reward)\n",
    "            if done:\n",
    "                break \n",
    "        scores_deque.append(sum(rewards))\n",
    "        scores.append(sum(rewards))\n",
    "        \n",
    "        discounts = [gamma**i for i in range(len(rewards)+1)]\n",
    "        R = sum([a*b for a,b in zip(discounts, rewards)])\n",
    "        \n",
    "        policy_loss = []\n",
    "        for log_prob in saved_log_probs:\n",
    "            policy_loss.append(-log_prob * R)\n",
    "        policy_loss = torch.cat(policy_loss).sum()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        optimizer.step()\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_deque)), end=\"\")\n",
    "        if i_episode % print_every == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_deque)))\n",
    "            torch.save(policy.state_dict(), f'{ENV}.pth')\n",
    "        if np.mean(scores_deque)>=REWARD_MAX:\n",
    "            print('Environment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_deque)))\n",
    "            torch.save(policy.state_dict(), f'{ENV}.pth')\n",
    "            break\n",
    "        \n",
    "    return scores\n",
    "    \n",
    "scores = reinforce()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a802efdf",
   "metadata": {},
   "source": [
    "* Hence very inefficient and not used. \n",
    "* There is no clear credit assignment. A trajectory may contain many good/bad actions and whether or not these actions are reinforced depends only on the final total output."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e62da970",
   "metadata": {},
   "source": [
    "Motivation for PPO\n",
    "* Because we have a Markov process, the action at time-step t, can only affect the future reward, so the past reward shouldn’t be contributing to the policy gradient.\n",
    "* The easiest option to reduce the noise in the gradient is to simply sample more trajectories! Using distributed computing, we can collect multiple trajectories in parallel, so that it won’t take too much time. Then we can estimate the policy gradient by averaging across all the different trajectories\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
