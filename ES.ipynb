{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evolution Strategies\n",
    "\n",
    "$\\alpha$ is the Learning rate, $\\sigma$ is the noise std deviation and $\\theta_0$ is the initial policy parameters\n",
    "- We sample a batch of noise $\\epsilon_i$, $i \\in \\N(0,1)$\n",
    "- Compute Returns $R_i = R(\\theta_t + \\sigma \\epsilon_i)$\n",
    "- Update the weights as : $\\theta_{t+1} = \\theta_t + \\alpha \\frac{1}{n \\sigma} \\sum_{i=1}^n R_i \\epsilon_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_BATCH_EPISODES = 100\n",
    "MAX_BATCH_STEPS = 10000\n",
    "NOISE_STD = 0.01\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, obs_size, action_size):\n",
    "        super(Net, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_size, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, action_size),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(env, net):\n",
    "    obs, _ = env.reset()\n",
    "    reward = 0.0\n",
    "    steps = 0\n",
    "    while True:\n",
    "        obs_v = torch.FloatTensor([obs])\n",
    "        act_prob = net(obs_v)\n",
    "        acts = act_prob.max(dim=1)[1]\n",
    "        obs, r, done, truncated, _ = env.step(acts.data.numpy()[0])\n",
    "        done = done | truncated\n",
    "        reward += r\n",
    "        steps += 1\n",
    "        if done:\n",
    "            break\n",
    "    return reward, steps\n",
    "\n",
    "def sample_noise(net):\n",
    "    \"\"\"Calculate a random noise and a negative noise for mirrored sampling\"\"\"\n",
    "    pos = []\n",
    "    neg = []\n",
    "    for p in net.parameters():\n",
    "        noise_t = torch.from_numpy(np.random.normal(size=p.data.size()).astype(np.float32))\n",
    "        pos.append(noise_t)\n",
    "        neg.append(-noise_t)\n",
    "    return pos, neg\n",
    "\n",
    "def eval_with_noise(env, net, noise):\n",
    "    \"\"\"Compute Returns $R_i = R(\\theta_t + \\sigma \\epsilon_i)$\"\"\"\n",
    "    old_params = net.state_dict()\n",
    "    for p, p_n in zip(net.parameters(), noise):\n",
    "        p.data += NOISE_STD * p_n\n",
    "    r, s = evaluate(env, net)\n",
    "    net.load_state_dict(old_params)\n",
    "    return r, s\n",
    "\n",
    "def train_step(net, batch_noise, batch_reward, writer, step_idx):\n",
    "    weighted_noise = None\n",
    "    norm_reward = np.array(batch_reward)\n",
    "    norm_reward -= np.mean(norm_reward)\n",
    "    s = np.std(norm_reward)\n",
    "    if abs(s) > 1e-6:\n",
    "        norm_reward /= s\n",
    "\n",
    "    for noise, reward in zip(batch_noise, norm_reward):\n",
    "        if weighted_noise is None:\n",
    "            weighted_noise = [reward * p_n for p_n in noise]\n",
    "        else:\n",
    "            for w_n, p_n in zip(weighted_noise, noise):\n",
    "                w_n += reward * p_n\n",
    "    m_updates = []\n",
    "    for p, p_update in zip(net.parameters(), weighted_noise):\n",
    "        update = p_update / (len(batch_reward) * NOISE_STD)\n",
    "        p.data += LEARNING_RATE * update\n",
    "        m_updates.append(torch.norm(update))\n",
    "    writer.add_scalar(\"update_l2\", np.mean(m_updates), step_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/pytorch/lib/python3.9/site-packages/gym/envs/registration.py:555: UserWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n",
      "  logger.warn(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/pytorch/lib/python3.9/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=4, out_features=32, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=32, out_features=2, bias=True)\n",
      "    (3): Softmax(dim=1)\n",
      "  )\n",
      ")\n",
      "1: reward=9.34, speed=15046.72 f/s\n",
      "2: reward=9.26, speed=17307.45 f/s\n",
      "3: reward=9.36, speed=16948.38 f/s\n",
      "4: reward=9.40, speed=17628.25 f/s\n",
      "5: reward=9.43, speed=17670.83 f/s\n",
      "6: reward=9.35, speed=17564.64 f/s\n",
      "7: reward=9.23, speed=17755.73 f/s\n",
      "8: reward=9.43, speed=17852.25 f/s\n",
      "9: reward=9.39, speed=17672.25 f/s\n",
      "10: reward=9.28, speed=17613.80 f/s\n",
      "11: reward=9.27, speed=17647.84 f/s\n",
      "12: reward=9.43, speed=17771.32 f/s\n",
      "13: reward=9.35, speed=17960.27 f/s\n",
      "14: reward=9.43, speed=18146.71 f/s\n",
      "15: reward=9.32, speed=18026.71 f/s\n",
      "16: reward=9.31, speed=17982.86 f/s\n",
      "17: reward=9.29, speed=17906.90 f/s\n",
      "18: reward=9.23, speed=17980.61 f/s\n",
      "19: reward=9.32, speed=17603.97 f/s\n",
      "20: reward=9.38, speed=17594.07 f/s\n",
      "21: reward=9.36, speed=17988.44 f/s\n",
      "22: reward=9.38, speed=17868.69 f/s\n",
      "23: reward=9.30, speed=17923.35 f/s\n",
      "24: reward=9.39, speed=18023.82 f/s\n",
      "25: reward=9.45, speed=17918.30 f/s\n",
      "26: reward=9.48, speed=18036.21 f/s\n",
      "27: reward=9.69, speed=18261.03 f/s\n",
      "28: reward=10.20, speed=18472.92 f/s\n",
      "29: reward=13.11, speed=19632.93 f/s\n",
      "30: reward=44.65, speed=22693.25 f/s\n",
      "31: reward=45.43, speed=22719.77 f/s\n",
      "32: reward=38.53, speed=22126.27 f/s\n",
      "33: reward=13.62, speed=19472.54 f/s\n",
      "34: reward=38.98, speed=22242.25 f/s\n",
      "35: reward=10.69, speed=18614.92 f/s\n",
      "36: reward=88.20, speed=23015.15 f/s\n",
      "37: reward=89.88, speed=23061.47 f/s\n",
      "38: reward=27.25, speed=21623.97 f/s\n",
      "39: reward=83.79, speed=23326.55 f/s\n",
      "40: reward=24.05, speed=21009.92 f/s\n",
      "41: reward=125.65, speed=23485.37 f/s\n",
      "42: reward=46.79, speed=22697.66 f/s\n",
      "43: reward=14.05, speed=19965.47 f/s\n",
      "44: reward=153.58, speed=23693.09 f/s\n",
      "45: reward=11.53, speed=18897.47 f/s\n",
      "46: reward=23.58, speed=21128.35 f/s\n",
      "47: reward=91.46, speed=23450.70 f/s\n",
      "48: reward=38.50, speed=22066.39 f/s\n",
      "49: reward=61.81, speed=23052.47 f/s\n",
      "50: reward=76.41, speed=23166.66 f/s\n",
      "51: reward=53.30, speed=22417.75 f/s\n",
      "52: reward=109.86, speed=22948.08 f/s\n",
      "53: reward=46.53, speed=22190.85 f/s\n",
      "54: reward=88.52, speed=22790.12 f/s\n",
      "55: reward=39.59, speed=21178.99 f/s\n",
      "56: reward=175.76, speed=22448.31 f/s\n",
      "57: reward=44.97, speed=21663.36 f/s\n",
      "58: reward=174.47, speed=23573.57 f/s\n",
      "59: reward=108.90, speed=23558.97 f/s\n",
      "60: reward=90.07, speed=23199.33 f/s\n",
      "61: reward=142.36, speed=23378.02 f/s\n",
      "62: reward=172.80, speed=23760.20 f/s\n",
      "63: reward=71.54, speed=22839.78 f/s\n",
      "64: reward=197.56, speed=23799.26 f/s\n",
      "Solved in 65 steps\n"
     ]
    }
   ],
   "source": [
    "writer = SummaryWriter(comment=\"-cartpole-es\")\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "\n",
    "net = Net(env.observation_space.shape[0], env.action_space.n)\n",
    "print(net)\n",
    "\n",
    "step_idx = 0\n",
    "while True:\n",
    "    t_start = time.time()\n",
    "    batch_noise = []\n",
    "    batch_reward = []\n",
    "    batch_steps = 0\n",
    "    for _ in range(MAX_BATCH_EPISODES):\n",
    "        noise, neg_noise = sample_noise(net)\n",
    "        batch_noise.append(noise)\n",
    "        batch_noise.append(neg_noise)\n",
    "        reward, steps = eval_with_noise(env, net, noise)\n",
    "        batch_reward.append(reward)\n",
    "        batch_steps += steps\n",
    "        reward, steps = eval_with_noise(env, net, neg_noise)\n",
    "        batch_reward.append(reward)\n",
    "        batch_steps += steps\n",
    "        if batch_steps > MAX_BATCH_STEPS:\n",
    "            break\n",
    "\n",
    "    step_idx += 1\n",
    "    m_reward = np.mean(batch_reward)\n",
    "    if m_reward > 199:\n",
    "        print(\"Solved in %d steps\" % step_idx)\n",
    "        break\n",
    "\n",
    "    train_step(net, batch_noise, batch_reward, writer, step_idx)\n",
    "    writer.add_scalar(\"reward_mean\", m_reward, step_idx)\n",
    "    writer.add_scalar(\"reward_std\", np.std(batch_reward), step_idx)\n",
    "    writer.add_scalar(\"reward_max\", np.max(batch_reward), step_idx)\n",
    "    writer.add_scalar(\"batch_episodes\", len(batch_reward), step_idx)\n",
    "    writer.add_scalar(\"batch_steps\", batch_steps, step_idx)\n",
    "    speed = batch_steps / (time.time() - t_start)\n",
    "    writer.add_scalar(\"speed\", speed, step_idx)\n",
    "    print(\"%d: reward=%.2f, speed=%.2f f/s\" % (step_idx, m_reward, speed))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
